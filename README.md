# module-11-challenge
data_collection

### Tools to explore and scrape websites:

- **Splinter**, a tool that automates our web browser actions, which allows us to automatically scan and repeat interactions on websites.

- **ChromeDriver**, which enables automation in the Chrome browser.

- **Beautiful Soup**, a Python library that allows you to pull out and parse specific information from a webpage.

html5lib and lxml, which are packages that Beautiful Soup uses to parse websites.

#### shell commands to complete the rest of the installations:

    `pip install "splinter[selenium4]"`
    `pip install bs4`
    `pip install html5lib`
    `pip install lxml`
    `pip install webdriver-manager`

1. Identify HTML components in a website.

2. Create a basic HTML document.

3. Scrape data from a website by using BeautifulSoup.

4. Style HTML elements by using CSS.

5. Use Splinter to perform automated browser actions.

6. Automate the web scraping process by using Splinter and Beautiful Soup.

7. Organize scraped information into a Python data structure.


---
--

 [web scraping](https://topwebscrapingservice.wordpress.com/2016/06/14/customized-web-data-scraping-services/)


![web scraping](https://www.edureka.co/blog/wp-content/uploads/2018/11/Untitled-1.jpg)